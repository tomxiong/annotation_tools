#!/usr/bin/env python3
"""
Main Test Runner for Panoramic Annotation GUI Functionality Testing

This script provides an easy-to-use interface for running comprehensive
automated functionality tests for the panoramic annotation GUI system.

Usage:
    python run_functionality_tests.py
    python run_functionality_tests.py --quick
    python run_functionality_tests.py --test-data-only
    python run_functionality_tests.py --report-only
"""

import argparse
import sys
import os
import time
import json
from pathlib import Path
from datetime import datetime

# Add project path to Python path
project_root = Path(__file__).parent.absolute()
src_path = project_root / 'src'
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

# Import the automated functionality tester
from automated_functionality_tester import AutomatedFunctionalityTester


def print_banner():
    """Print application banner"""
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    Panoramic Annotation GUI Testing Framework                â•‘
â•‘                          Automated Functionality Tester                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”¬ Comprehensive functionality detection and validation system
ğŸ›   Intelligent defect detection with auto-resolution capabilities  
ğŸ“Š Performance monitoring and optimization
ğŸ¯ Test data management for panoramic image library
"""
    print(banner)


def run_quick_tests(test_image_dir: str = "D:\\test\\images"):
    """Run quick functionality tests (limited cycles)"""
    print("ğŸš€ Running Quick Functionality Tests")
    print("=" * 60)
    
    tester = AutomatedFunctionalityTester(test_image_dir)
    tester.max_cycles = 3  # Limit cycles for quick test
    
    start_time = time.time()
    results = tester.execute_comprehensive_functionality_test()
    end_time = time.time()
    
    print_test_summary(results, end_time - start_time, "Quick Test")
    return results


def run_comprehensive_tests(test_image_dir: str = "D:\\test\\images"):
    """Run comprehensive functionality tests (full cycles)"""
    print("ğŸ” Running Comprehensive Functionality Tests")
    print("=" * 60)
    
    tester = AutomatedFunctionalityTester(test_image_dir)
    tester.max_cycles = 10  # Full testing cycles
    
    start_time = time.time()
    results = tester.execute_comprehensive_functionality_test()
    end_time = time.time()
    
    print_test_summary(results, end_time - start_time, "Comprehensive Test")
    return results


def run_panoramic_workflow_tests(test_image_dir: str = "D:\\test\\images"):
    """Run panoramic annotation workflow tests"""
    print("ğŸ¯ Running Panoramic Annotation Workflow Tests")
    print("=" * 60)
    
    try:
        # Import and run the panoramic workflow test
        sys.path.append(str(project_root / "tools" / "testing"))
        from test_panoramic_workflow import run_comprehensive_workflow_test
        
        start_time = time.time()
        success = run_comprehensive_workflow_test(test_image_dir)
        end_time = time.time()
        
        execution_time = end_time - start_time
        print(f"\nâ±ï¸  Panoramic Workflow Test Execution Time: {execution_time:.2f} seconds")
        
        return {"passed": success, "execution_time": execution_time}
        
    except Exception as e:
        print(f"âŒ Panoramic workflow tests failed: {e}")
        return {"passed": False, "execution_time": 0, "error": str(e)}


def create_test_data_only(test_image_dir: str = "D:\\test\\images"):
    """Create test data without running tests"""
    print("ğŸ“ Creating Test Data Only")
    print("=" * 60)
    
    tester = AutomatedFunctionalityTester(test_image_dir)
    
    # Just ensure test data availability
    data_created = tester.test_data_manager.ensure_test_data_availability()
    
    if data_created:
        print(f"âœ… Test data successfully created/verified at: {test_image_dir}")
        
        # List created files
        test_dir = Path(test_image_dir)
        if test_dir.exists():
            files = list(test_dir.glob("*"))
            print(f"\nğŸ“‹ Created files ({len(files)} total):")
            for file in sorted(files)[:10]:  # Show first 10 files
                print(f"   ğŸ“„ {file.name}")
            if len(files) > 10:
                print(f"   ... and {len(files) - 10} more files")
    else:
        print(f"âŒ Failed to create test data at: {test_image_dir}")
    
    return data_created


def generate_report_only():
    """Generate report from existing test results"""
    print("ğŸ“Š Generating Report from Existing Results")
    print("=" * 60)
    
    reports_dir = project_root / "test_reports"
    
    if not reports_dir.exists():
        print("âŒ No test reports directory found. Run tests first.")
        return False
    
    # Find latest report
    report_files = list(reports_dir.glob("functionality_test_report_*.json"))
    
    if not report_files:
        print("âŒ No test reports found. Run tests first.")
        return False
    
    latest_report = max(report_files, key=lambda x: x.stat().st_mtime)
    
    try:
        with open(latest_report, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        print(f"ğŸ“‹ Report from: {latest_report.name}")
        print_test_summary(results, results.get('total_execution_time', 0), "Report")
        
        return True
        
    except Exception as e:
        print(f"âŒ Failed to load report: {e}")
        return False


def print_test_summary(results: dict, execution_time: float, test_type: str):
    """Print comprehensive test summary"""
    print(f"\n{'='*60}")
    print(f"ğŸ¯ {test_type.upper()} RESULTS SUMMARY")
    print(f"{'='*60}")
    
    # Basic information
    print(f"ğŸ“… Session ID: {results.get('session_id', 'N/A')}")
    print(f"â±ï¸  Total Execution Time: {execution_time:.2f} seconds")
    print(f"ğŸ”„ Test Cycles: {results.get('test_cycles', 0)}/{results.get('max_cycles', 0)}")
    print(f"ğŸ­ Overall Status: {results.get('overall_functionality_status', 'UNKNOWN')}")
    
    # Defect information
    defects_detected = results.get('defects_detected', 0)
    defects_resolved = results.get('defects_resolved', 0)
    auto_fixes = results.get('auto_fixes_applied', 0)
    
    print(f"\nğŸ”§ DEFECT ANALYSIS:")
    print(f"   ğŸš¨ Defects Detected: {defects_detected}")
    print(f"   âœ… Defects Resolved: {defects_resolved}")
    print(f"   ğŸ› ï¸  Auto-fixes Applied: {auto_fixes}")
    
    if defects_detected > 0:
        resolution_rate = (defects_resolved / defects_detected) * 100
        print(f"   ğŸ“Š Resolution Rate: {resolution_rate:.1f}%")
    
    # Phase results
    print(f"\nğŸ“‹ PHASE RESULTS:")
    phase_results = results.get('results', {})
    
    for phase, result in phase_results.items():
        status = "âœ… PASS" if result.get('passed', False) else "âŒ FAIL"
        exec_time = result.get('execution_time', 0)
        defects = result.get('defects_found', 0)
        fixes = result.get('fixes_applied', 0)
        success_rate = result.get('success_rate', 0) * 100
        
        print(f"   {status} {phase.replace('_', ' ').title()}")
        print(f"      â±ï¸  Time: {exec_time:.2f}s | ğŸš¨ Defects: {defects} | ğŸ› ï¸  Fixes: {fixes} | ğŸ“Š Success: {success_rate:.1f}%")
    
    # Performance metrics
    perf_metrics = results.get('performance_metrics', {})
    if perf_metrics:
        print(f"\nâš¡ PERFORMANCE METRICS:")
        
        memory_mb = perf_metrics.get('memory_usage_mb', 0)
        if memory_mb > 0:
            print(f"   ğŸ’¾ Memory Usage: {memory_mb:.1f} MB")
        
        cpu_percent = perf_metrics.get('cpu_usage_percent', 0)
        if cpu_percent > 0:
            print(f"   ğŸ’» CPU Usage: {cpu_percent:.1f}%")
        
        startup_time = perf_metrics.get('startup_time_s', 0)
        if startup_time > 0:
            print(f"   ğŸš€ Startup Time: {startup_time:.1f}s")
    
    # Overall assessment
    overall_status = results.get('overall_functionality_status', 'UNKNOWN')
    
    if overall_status == 'FUNCTIONAL':
        print(f"\nğŸ‰ ASSESSMENT: System is fully functional!")
        print("   All functionality tests passed successfully.")
        print("   The panoramic annotation GUI is ready for use.")
    elif overall_status == 'DEFECTS_DETECTED':
        print(f"\nâš ï¸ ASSESSMENT: Some issues detected but may be resolved.")
        print("   Check individual phase results for details.")
        print("   Consider running tests again or manual investigation.")
    else:
        print(f"\nâ“ ASSESSMENT: Test status unclear.")
        print("   Check test logs for more information.")
    
    # Next steps
    print(f"\nğŸ“‹ NEXT STEPS:")
    if overall_status == 'FUNCTIONAL':
        print("   âœ… System ready for production use")
        print("   ğŸ“Š Consider performance monitoring")
        print("   ğŸ”„ Run periodic functionality checks")
    else:
        print("   ğŸ” Review detailed test logs")
        print("   ğŸ› ï¸  Apply manual fixes if needed")
        print("   ğŸ”„ Re-run tests after fixes")
    
    # File locations
    print(f"\nğŸ“ FILES GENERATED:")
    logs_dir = project_root / "test_logs"
    reports_dir = project_root / "test_reports"
    
    if logs_dir.exists():
        print(f"   ğŸ“ Test Logs: {logs_dir}")
    if reports_dir.exists():
        print(f"   ğŸ“Š Test Reports: {reports_dir}")
    
    print(f"{'='*60}")


def main():
    """Main function"""
    parser = argparse.ArgumentParser(
        description="Main Test Runner for Panoramic Annotation GUI Functionality Testing",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        '--quick',
        action='store_true',
        help='Run quick functionality tests (limited cycles)'
    )
    
    parser.add_argument(
        '--test-data-only',
        action='store_true',
        help='Create test data without running tests'
    )
    
    parser.add_argument(
        '--report-only',
        action='store_true',
        help='Generate report from existing test results'
    )
    
    parser.add_argument(
        '--panoramic-workflow',
        action='store_true',
        help='Run panoramic annotation workflow tests only'
    )
    
    parser.add_argument(
        '--test-dir',
        type=str,
        default="D:\\test\\images",
        help='Test image directory path (default: D:\\test\\images)'
    )
    
    args = parser.parse_args()
    
    print_banner()
    
    try:
        if args.test_data_only:
            success = create_test_data_only(args.test_dir)
        elif args.report_only:
            success = generate_report_only()
        elif args.panoramic_workflow:
            results = run_panoramic_workflow_tests(args.test_dir)
            success = results.get("passed", False)
        elif args.quick:
            results = run_quick_tests(args.test_dir)
            success = results.get("overall_functionality_status") == "FUNCTIONAL"
        else:
            results = run_comprehensive_tests(args.test_dir)
            success = results.get("overall_functionality_status") == "FUNCTIONAL"
        
        if success:
            print("\nâœ… All tests completed successfully!")
            sys.exit(0)
        else:
            print("\nâŒ Some tests failed!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Tests interrupted by user.")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Unexpected error: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()